"""
Query Normalization - Xá»­ lÃ½ tá»« lÃ³ng vÃ  viáº¿t táº¯t
Chuáº©n hÃ³a query tá»« sinh viÃªn sang ngÃ´n ngá»¯ chuáº©n trÆ°á»›c khi xá»­ lÃ½
"""

import re
from typing import Dict, List, Tuple


class QueryNormalizer:
    """Chuáº©n hÃ³a query Ä‘á»ƒ hiá»ƒu tá»« lÃ³ng vÃ  viáº¿t táº¯t"""
    
    # Tá»« Ä‘iá»ƒn viáº¿t táº¯t phá»• biáº¿n
    ABBREVIATIONS = {
        # Viáº¿t táº¯t chung
        "sv": "sinh viÃªn",
        "gv": "giáº£ng viÃªn",
        "cb": "cÃ¡n bá»™",
        "hs": "há»c sinh",
        
        # ÄÃ o táº¡o
        "Ä‘ktc": "Ä‘Äƒng kÃ½ tÃ­n chá»‰",
        "Ä‘khp": "Ä‘Äƒng kÃ½ há»c pháº§n",
        "tc": "tÃ­n chá»‰",
        "hp": "há»c pháº§n",
        "hk": "há»c ká»³",
        "ctÄ‘t": "chÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o",
        "tn": "tá»‘t nghiá»‡p",
        "xltn": "xÃ©t tá»‘t nghiá»‡p",
        "bv": "báº£o vá»‡",
        "kltn": "khÃ³a luáº­n tá»‘t nghiá»‡p",
        "Ä‘atn": "Ä‘á»“ Ã¡n tá»‘t nghiá»‡p",
        
        # Äiá»ƒm sá»‘
        "dtb": "Ä‘iá»ƒm trung bÃ¬nh",
        "Ä‘tbhk": "Ä‘iá»ƒm trung bÃ¬nh há»c ká»³",
        "Ä‘tbtl": "Ä‘iá»ƒm trung bÃ¬nh tÃ­ch lÅ©y",
        "gpa": "grade point average",
        "cpa": "cumulative point average",
        
        # Thá»§ tá»¥c
        "Ä‘Ä‘": "Ä‘Äƒng kÃ½",
        "Ä‘k": "Ä‘Äƒng kÃ½",
        "nv": "nhÃ  vá»‡ sinh",  # joke, remove this
        "ktx": "kÃ½ tÃºc xÃ¡",
        "bhyt": "báº£o hiá»ƒm y táº¿",
        "bhtn": "báº£o hiá»ƒm tháº¥t nghiá»‡p",
        
        # KhÃ¡c
        "qc": "quy cháº¿",
        "qÄ‘": "quyáº¿t Ä‘á»‹nh",
        "cv": "cÃ´ng vÄƒn",
        "tb": "thÃ´ng bÃ¡o",
    }
    
    # Tá»« lÃ³ng sinh viÃªn
    SLANG_TERMS = {
        # Há»c táº­p
        "rá»›t mÃ´n": "Ä‘iá»ƒm f",
        "trÆ°á»£t mÃ´n": "Ä‘iá»ƒm f",
        "trÆ°á»£t": "khÃ´ng Ä‘áº¡t",
        "pass": "Ä‘áº¡t",
        "Ä‘áº­u": "Ä‘áº¡t",
        "Äƒn Ä‘iá»ƒm": "há»c láº¡i",
        "há»c láº¡i": "Ä‘Äƒng kÃ½ há»c láº¡i",
        "cÃ y cuá»‘c": "há»c táº­p chÄƒm chá»‰",
        "cÃ y": "há»c chÄƒm",
        "gÃ ": "Ä‘iá»ƒm tháº¥p",
        "gÃ  má»": "Ä‘iá»ƒm kÃ©m",
        
        # Äiá»ƒm sá»‘
        "Ä‘iá»ƒm khá»§ng": "Ä‘iá»ƒm cao",
        "Ä‘iá»ƒm cao": "Ä‘iá»ƒm a",
        "Ä‘iá»ƒm giá»i": "Ä‘iá»ƒm a",
        "Ä‘iá»ƒm khÃ¡": "Ä‘iá»ƒm b",
        "Ä‘iá»ƒm tb": "Ä‘iá»ƒm c",
        "Ä‘iá»ƒm yáº¿u": "Ä‘iá»ƒm d",
        "Ä‘iá»ƒm kÃ©m": "Ä‘iá»ƒm f",
        "bay mÃ u": "Ä‘iá»ƒm f",
        "toang": "Ä‘iá»ƒm f",
        
        # Thá»§ tá»¥c
        "Ä‘Äƒng kÃ½ mÃ´n": "Ä‘Äƒng kÃ½ há»c pháº§n",
        "Ä‘k mÃ´n": "Ä‘Äƒng kÃ½ há»c pháº§n",
        "rÃºt mÃ´n": "rÃºt bá»›t há»c pháº§n",
        "bá» mÃ´n": "rÃºt bá»›t há»c pháº§n",
        "nghá»‰ há»c": "báº£o lÆ°u",
        "nghá»‰ táº¡m": "báº£o lÆ°u táº¡m thá»i",
        "xin nghá»‰": "Ä‘Æ¡n xin nghá»‰ há»c",
        
        # Tá»‘t nghiá»‡p
        "ra trÆ°á»ng": "tá»‘t nghiá»‡p",
        "tá»‘t nghiá»‡p": "hoÃ n thÃ nh chÆ°Æ¡ng trÃ¬nh",
        "nháº­n báº±ng": "cáº¥p báº±ng tá»‘t nghiá»‡p",
        "báº£o vá»‡": "báº£o vá»‡ khÃ³a luáº­n",
        "bv Ä‘á»“ Ã¡n": "báº£o vá»‡ Ä‘á»“ Ã¡n tá»‘t nghiá»‡p",
        
        # KhÃ¡c
        "tháº§y": "giáº£ng viÃªn",
        "cÃ´": "giáº£ng viÃªn",
        "phÃ²ng Ä‘Ã o táº¡o": "phÃ²ng quáº£n lÃ½ Ä‘Ã o táº¡o",
        "vÄƒn phÃ²ng khoa": "phÃ²ng Ä‘Ã o táº¡o",
    }
    
    # CÃ¡c cá»¥m tá»« Ä‘á»“ng nghÄ©a
    SYNONYMS = {
        "Ä‘iá»u kiá»‡n tá»‘t nghiá»‡p": ["xÃ©t tá»‘t nghiá»‡p", "ra trÆ°á»ng", "nháº­n báº±ng"],
        "Ä‘Äƒng kÃ½ há»c pháº§n": ["Ä‘Äƒng kÃ½ mÃ´n", "Ä‘k mÃ´n", "Ä‘ktc", "Ä‘khp"],
        "Ä‘iá»ƒm f": ["rá»›t mÃ´n", "trÆ°á»£t", "khÃ´ng Ä‘áº¡t", "bay mÃ u", "toang"],
        "há»c láº¡i": ["Äƒn Ä‘iá»ƒm", "thi láº¡i", "Ä‘Äƒng kÃ½ láº¡i"],
        "báº£o lÆ°u": ["nghá»‰ há»c", "táº¡m nghá»‰", "nghá»‰ táº¡m thá»i"],
    }
    
    def __init__(self):
        """Initialize normalizer"""
        # Combine all mappings
        self.normalization_map = {
            **self.ABBREVIATIONS,
            **self.SLANG_TERMS
        }
        
        # Build regex pattern for efficient replacement
        self._build_pattern()
    
    def _build_pattern(self):
        """Build regex pattern from all terms"""
        # Sort by length (longest first) to match longer phrases first
        terms = sorted(self.normalization_map.keys(), key=len, reverse=True)
        
        # Escape special regex characters and join with |
        pattern = '|'.join(re.escape(term) for term in terms)
        self.pattern = re.compile(r'\b(' + pattern + r')\b', re.IGNORECASE)
    
    def normalize(self, query: str) -> str:
        """
        Chuáº©n hÃ³a query báº±ng cÃ¡ch thay tháº¿ tá»« lÃ³ng/viáº¿t táº¯t
        
        Args:
            query: CÃ¢u há»i gá»‘c tá»« user
            
        Returns:
            CÃ¢u há»i Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a
        """
        normalized = query
        
        # Replace using regex for case-insensitive matching
        def replace_func(match):
            matched_text = match.group(1)
            # Find the key in normalization_map (case-insensitive)
            for key, value in self.normalization_map.items():
                if key.lower() == matched_text.lower():
                    return value
            return matched_text
        
        normalized = self.pattern.sub(replace_func, normalized)
        
        return normalized
    
    def get_explanation(self, query: str) -> List[Tuple[str, str]]:
        """
        Tráº£ vá» list cÃ¡c tá»« Ä‘Ã£ Ä‘Æ°á»£c normalize vÃ  Ã½ nghÄ©a
        
        Args:
            query: CÃ¢u há»i gá»‘c
            
        Returns:
            List of (original_term, normalized_term) tuples
        """
        explanations = []
        
        for match in self.pattern.finditer(query):
            matched_text = match.group(1)
            for key, value in self.normalization_map.items():
                if key.lower() == matched_text.lower():
                    explanations.append((matched_text, value))
                    break
        
        return explanations
    
    def add_custom_term(self, slang: str, standard: str):
        """
        ThÃªm tá»« lÃ³ng/viáº¿t táº¯t custom
        
        Args:
            slang: Tá»« lÃ³ng hoáº·c viáº¿t táº¯t
            standard: Tá»« chuáº©n
        """
        self.normalization_map[slang.lower()] = standard.lower()
        self._build_pattern()


# Global normalizer instance
normalizer = QueryNormalizer()


if __name__ == "__main__":
    # Test
    test_queries = [
        "sv rá»›t mÃ´n pháº£i lÃ m gÃ¬",
        "Ä‘ktc nhÆ° tháº¿ nÃ o",
        "Ä‘iá»u kiá»‡n tn lÃ  gÃ¬",
        "tÃ´i bá»‹ bay mÃ u 3 mÃ´n, há»c láº¡i Ä‘Æ°á»£c khÃ´ng",
        "gpa tháº¥p cÃ³ ra trÆ°á»ng Ä‘Æ°á»£c khÃ´ng",
        "Ä‘k mÃ´n cho hk sau",
        "cÃ y cuá»‘c cáº£ ká»³ váº«n gÃ ",
        "tháº§y cho Ä‘iá»ƒm f, tÃ´i pháº£i Äƒn Ä‘iá»ƒm Ã ",
        "bv kltn cáº§n gÃ¬"
    ]
    
    print("ğŸ”§ Testing Query Normalizer\n")
    print("="*60)
    
    for query in test_queries:
        normalized = normalizer.normalize(query)
        explanations = normalizer.get_explanation(query)
        
        print(f"\nğŸ“ Original:   {query}")
        print(f"âœ… Normalized: {normalized}")
        
        if explanations:
            print(f"ğŸ“– Terms normalized:")
            for original, standard in explanations:
                print(f"   â€¢ '{original}' â†’ '{standard}'")
        
        print("-"*60)
