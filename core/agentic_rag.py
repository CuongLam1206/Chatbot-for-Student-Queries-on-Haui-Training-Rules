"""
Agentic RAG System - Main Implementation
Há»‡ thá»‘ng RAG vá»›i agents thÃ´ng minh sá»­ dá»¥ng LangGraph
"""

from typing import Dict, Any, List, Literal
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, END
import os

from .config import model_config, vectorstore_config, agent_config, system_config
from .agents import (
    AgentState,
    QueryAnalyzerAgent,
    RetrievalPlannerAgent,
    RetrievalAgent,
    ReasoningAgent,
    ValidationAgent,
    ResponseFormatterAgent
)


class AgenticRAG:
    """
    Agentic RAG System
    
    Workflow:
    1. Query Analysis - PhÃ¢n tÃ­ch cÃ¢u há»i
    2. Retrieval Planning - Láº­p káº¿ hoáº¡ch tÃ¬m kiáº¿m
    3. Retrieval - TÃ¬m kiáº¿m thÃ´ng tin
    4. Reasoning - Suy luáº­n tráº£ lá»i
    5. Validation - Kiá»ƒm tra cháº¥t lÆ°á»£ng
    6. Response Formatting - Format cÃ¢u tráº£ lá»i
    """
    
    def __init__(self, vectorstore: Chroma = None):
        """
        Khá»Ÿi táº¡o Agentic RAG System
        
        Args:
            vectorstore: Chroma vectorstore (náº¿u None, sáº½ load tá»« config)
        """
        # Initialize LLM
        self.llm = ChatOpenAI(
            model=model_config.chat_model,
            temperature=model_config.temperature,
            max_tokens=model_config.max_tokens,
            api_key=model_config.openai_api_key
        )
        
        # Initialize or load vectorstore
        if vectorstore is None:
            embeddings = HuggingFaceEmbeddings(
                model_name=model_config.embedding_model
            )
            
            if os.path.exists(vectorstore_config.persist_directory):
                self.vectorstore = Chroma(
                    persist_directory=vectorstore_config.persist_directory,
                    embedding_function=embeddings
                )
                print(f"âœ… Loaded vectorstore from {vectorstore_config.persist_directory}")
            else:
                raise ValueError(f"Vectorstore not found at {vectorstore_config.persist_directory}. Please create it first.")
        else:
            self.vectorstore = vectorstore
        
        # Initialize agents
        self.query_analyzer = QueryAnalyzerAgent(self.llm)
        self.retrieval_planner = RetrievalPlannerAgent(self.llm)
        self.retrieval_agent = RetrievalAgent(self.vectorstore)
        self.reasoning_agent = ReasoningAgent(self.llm)
        self.validation_agent = ValidationAgent(self.llm)
        self.formatter = ResponseFormatterAgent()
        
        # Build workflow
        self.workflow = self._build_workflow()
        
        print("âœ… AgenticRAG initialized successfully")
    
    def _build_workflow(self) -> StateGraph:
        """XÃ¢y dá»±ng LangGraph workflow"""
        
        # Create graph
        workflow = StateGraph(AgentState)
        
        # Add nodes (cÃ¡c bÆ°á»›c trong workflow)
        workflow.add_node("analyze_query", self.query_analyzer.analyze)
        workflow.add_node("plan_retrieval", self.retrieval_planner.plan)
        workflow.add_node("retrieve", self.retrieval_agent.retrieve)
        workflow.add_node("reason", self.reasoning_agent.reason)
        workflow.add_node("validate", self.validation_agent.validate)
        workflow.add_node("format_response", self.formatter.format)
        workflow.add_node("direct_response", self._handle_direct_response)
        
        # Define edges (luá»“ng cháº¡y)
        workflow.set_entry_point("analyze_query")
        
        # Conditional edge: skip retrieval náº¿u lÃ  greeting/chitchat
        workflow.add_conditional_edges(
            "analyze_query",
            self._needs_retrieval,
            {
                "retrieval": "plan_retrieval",
                "direct": "direct_response"
            }
        )
        
        workflow.add_edge("plan_retrieval", "retrieve")
        workflow.add_edge("retrieve", "reason")
        workflow.add_edge("reason", "validate")
        
        # Conditional edge: retry náº¿u validation fail
        workflow.add_conditional_edges(
            "validate",
            self._should_retry,
            {
                "retry": "plan_retrieval",  # Retry tá»« planning
                "continue": "format_response"
            }
        )
        
        workflow.add_edge("format_response", END)
        workflow.add_edge("direct_response", END)
        
        # Compile
        return workflow.compile()
    
    def _needs_retrieval(self, state: AgentState) -> Literal["retrieval", "direct"]:
        """Quyáº¿t Ä‘á»‹nh cÃ³ cáº§n retrieval hay tráº£ lá»i trá»±c tiáº¿p"""
        analysis = state.get("query_analysis", {})
        needs_retrieval = analysis.get("needs_retrieval", True)
        
        if needs_retrieval:
            return "retrieval"
        else:
            return "direct"
    
    def _handle_direct_response(self, state: AgentState) -> AgentState:
        """Xá»­ lÃ½ direct response (khÃ´ng cáº§n retrieval)"""
        analysis = state.get("query_analysis", {})
        direct_response = analysis.get("direct_response", "")
        
        state["final_answer"] = direct_response
        state["confidence_score"] = 1.0
        state["citations"] = []
        state["current_step"] = "direct_response_completed"
        
        return state

    
    def _should_retry(self, state: AgentState) -> Literal["retry", "continue"]:
        """Quyáº¿t Ä‘á»‹nh cÃ³ retry hay khÃ´ng"""
        if state.get("needs_retry", False):
            # TÄƒng retry count
            state["retry_count"] = state.get("retry_count", 0) + 1
            return "retry"
        return "continue"
    
    def query(self, question: str, conversation_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        Xá»­ lÃ½ cÃ¢u há»i tá»« user
        
        Args:
            question: CÃ¢u há»i
            conversation_history: Lá»‹ch sá»­ há»™i thoáº¡i (optional)
        
        Returns:
            Dict chá»©a:
            - answer: CÃ¢u tráº£ lá»i
            - confidence: Äá»™ tin cáº­y
            - citations: Nguá»“n tham kháº£o
            - metadata: ThÃ´ng tin debug
        """
        if system_config.verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ¤– AGENTIC RAG PROCESSING")
            print(f"{'='*60}")
            print(f"Question: {question}")
        
        # Initialize state
        initial_state: AgentState = {
            "original_query": question,
            "conversation_history": conversation_history or [],
            "query_analysis": None,
            "reformulated_queries": [],
            "retrieved_documents": [],
            "retrieval_strategy": "",
            "reasoning_steps": [],
            "intermediate_answers": [],
            "final_answer": "",
            "confidence_score": 0.0,
            "citations": [],
            "validation_result": None,
            "needs_retry": False,
            "retry_count": 0,
            "current_step": "initialized",
            "error_message": None
        }
        
        try:
            # Run workflow vá»›i recursion limit cao hÆ¡n
            final_state = self.workflow.invoke(
                initial_state,
                {"recursion_limit": 50}  # TÄƒng tá»« 25 default
            )
            
            if system_config.verbose:
                print(f"\n{'='*60}")
                print(f"âœ… PROCESSING COMPLETE")
                print(f"{'='*60}\n")
            
            # Extract results
            return {
                "answer": final_state["final_answer"],
                "confidence": final_state["confidence_score"],
                "citations": final_state["citations"],
                "metadata": {
                    "query_analysis": final_state.get("query_analysis"),
                    "num_documents": len(final_state.get("retrieved_documents", [])),
                    "retrieval_strategy": final_state.get("retrieval_strategy"),
                    "retry_count": final_state.get("retry_count", 0),
                    "validation": final_state.get("validation_result")
                }
            }
            
        except Exception as e:
            error_msg = f"Error processing query: {str(e)}"
            print(f"âŒ {error_msg}")
            
            return {
                "answer": f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i: {str(e)}",
                "confidence": 0.0,
                "citations": [],
                "metadata": {
                    "error": error_msg
                }
            }
    
    def chat(self, message: str, history: List[List[str]]) -> str:
        """
        Interface cho Gradio chatbot
        
        Args:
            message: Tin nháº¯n tá»« user
            history: Lá»‹ch sá»­ chat [[user, bot], [user, bot], ...]
        
        Returns:
            CÃ¢u tráº£ lá»i
        """
        # Convert history to conversation format
        conversation_history = []
        for user_msg, bot_msg in history:
            conversation_history.append({"role": "user", "content": user_msg})
            conversation_history.append({"role": "assistant", "content": bot_msg})
        
        # Query
        result = self.query(message, conversation_history)
        
        return result["answer"]
    
    def print_config(self):
        """In ra cáº¥u hÃ¬nh hiá»‡n táº¡i"""
        from .config import get_config_summary
        import json
        
        print("\n" + "="*60)
        print("AGENTIC RAG CONFIGURATION")
        print("="*60)
        print(json.dumps(get_config_summary(), indent=2, ensure_ascii=False))
        print("="*60 + "\n")


def load_agentic_rag(vectorstore_path: str = None) -> AgenticRAG:
    """
    Tiá»‡n Ã­ch Ä‘á»ƒ load AgenticRAG system
    
    Args:
        vectorstore_path: ÄÆ°á»ng dáº«n Ä‘áº¿n vectorstore (optional)
    
    Returns:
        AgenticRAG instance
    """
    if vectorstore_path:
        # Load custom vectorstore
        embeddings = HuggingFaceEmbeddings(
            model_name=model_config.embedding_model
        )
        vectorstore = Chroma(
            persist_directory=vectorstore_path,
            embedding_function=embeddings
        )
        return AgenticRAG(vectorstore)
    else:
        # Load tá»« config
        return AgenticRAG()


if __name__ == "__main__":
    # Test the system
    print("ğŸš€ Testing Agentic RAG System\n")
    
    try:
        # Initialize
        agentic_rag = AgenticRAG()
        agentic_rag.print_config()
        
        # Test query
        test_question = "Sinh viÃªn bá»‹ Ä‘iá»ƒm F pháº£i lÃ m gÃ¬?"
        
        print(f"\nğŸ“ Test Question: {test_question}\n")
        
        result = agentic_rag.query(test_question)
        
        print(f"\nğŸ“„ Answer:\n{result['answer']}")
        print(f"\nğŸ¯ Confidence: {result['confidence']:.2%}")
        print(f"\nğŸ“š Citations: {', '.join(result['citations'])}")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("\nNote: Make sure you have created the vectorstore first by running the notebook!")
